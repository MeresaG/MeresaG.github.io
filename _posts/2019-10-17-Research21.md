---
title: 21. Streaming algorithm for the mean () and the variance (2) (Knuth, Welford). Show with an example why the running mean formula is preferable to the definition formula.
---
in the previous post, we saw what mean is and now we will introduce a new term variance or standard deviation.
The standard deviation is a measure of how much a dataset differs from its mean; it tells us how dispersed the data are. A dataset that’s pretty much clumped around a single point would have a small standard deviation, while a dataset that’s all over the map would have a large standard deviation.

Given a sample x1,…,xN , the standard deviation is defined as the square root of the variance:
      
 ![alt text here](/img/variance.png) 
 Here \overline{x} is the mean of the sample: \overline{x}=\frac{\sum_{i=1}^{N} x_i}{N}. The definition can be converted directly into an algorithm that computes the variance and standard deviation in two passes: compute the mean in one pass over the data, and then do a second pass to compute the squared differences from the mean. Doing two passes is not ideal though, and it can be impractical in some settings. For example, if the samples are generated by a random simulation it may be prohibitively expensive to store samples just so you can do a second pass over them.

                        
      
The most direct way of computing sample mean, variance or standard deviation can have severe numerical problems. 
