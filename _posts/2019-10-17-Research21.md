---
title: 21. Streaming algorithm for the mean () and the variance (2) (Knuth, Welford). Show with an example why the running mean formula is preferable to the definition formula.
---
in the previous post, we saw what mean is and now we will introduce a new term variance or standard deviation.
The standard deviation is a measure of how much a dataset differs from its mean; it tells us how dispersed the data are. A dataset that’s pretty much clumped around a single point would have a small standard deviation, while a dataset that’s all over the map would have a large standard deviation.

Given a sample x1,…,xN , the standard deviation is defined as the square root of the variance:
      
              ![alt text here](https://github.com/MeresaG/MeresaG.github.io/blob/master/img/variance.png)

                        
      
The most direct way of computing sample mean, variance or standard deviation can have severe numerical problems. 
